{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling sentiment analysis in MXNet with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is CNN\n",
    "\n",
    "A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers (often with a subsampling step) and then followed by one or more fully connected layers as in a standard multilayer neural network. The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal). This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features. Another benefit of CNNs is that they are easier to train and have many fewer parameters than fully connected networks with the same number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem \n",
    "\n",
    "Sentimenet ananlysis is the use of natual langurage processing(NLP) to determine the attitute expressed by an author in a piece of written text towards a topic, e.g. movie review.  The attitute can be positive, neutral, and negative.  \n",
    "\n",
    "From a machine learning perspective, sentiment analysis can be treated as a claffisication problem. In the tutorial, we will train a CNN based model for sentiment analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use This TutorialÂ¶\n",
    "You can use this tutorial by executing each snippet of python code in order as it appears in the notebook.\n",
    "In this tutorial, we will train a MLP on IMDB dataset which will ultimately produce a neural network that can predict the sentiment of movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Familiarity with MXNet, Python, Numpy, basics of MLP networks.\n",
    "AWS Deep Learning AMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "The training and testing dataset is the IMDB movie review database.  It contains a total of 50,000 movie reviews that are tagged (labelled) with either a negative(0) or a positive(1) sentiment.  We will split the dataset into 25,000 reviews for training and 25,000 reviews for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load all the libraries and modules\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from text import Tokenizer\n",
    "import mxnet as mx\n",
    "from matplotlib import pyplot\n",
    "from six.moves.urllib.request import urlopen\n",
    "from sequence import pad_sequences\n",
    "\n",
    "from IPython.display import display \n",
    "from IPython.html import widgets\n",
    "\n",
    "# Enable logging so we will see output during the training\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Movie Review Data\n",
    "\n",
    "The dataset can be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/. The file is then unzipped and processed into traing and testing datasets for training and validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# We specify number of words to index and this is also the size of vocabulary\n",
    "num_words = 10000\n",
    "\n",
    "# This is the directory where the raw review data is located\n",
    "path = \"../data/aclImdb/\"\n",
    "\n",
    "# List all the files for the reviews in the following directories\n",
    "ff = [path + \"train/pos/\" + x for x in os.listdir(path + \"train/pos\")] + \\\n",
    "     [path + \"train/neg/\" + x for x in os.listdir(path + \"train/neg\")] + \\\n",
    "     [path + \"test/pos/\" + x for x in os.listdir(path + \"test/pos\")] + \\\n",
    "     [path + \"test/neg/\" + x for x in os.listdir(path + \"test/neg\")]\n",
    "\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "input_label = ([1] * 12500 + [0] * 12500) * 2\n",
    "input_text  = []\n",
    "\n",
    "for f in ff:\n",
    "    with open(f) as fin:\n",
    "        pass\n",
    "        input_text += [remove_tags(\" \".join(fin.readlines()))]\n",
    "            \n",
    "# Initialize a tokenizer with the vocabulary size and train on data input text to create a vocabulary for all \n",
    "# the unique words found in the text inputs\n",
    "tok = Tokenizer(num_words)\n",
    "tok.fit_on_texts(input_text[:25000])\n",
    "    \n",
    "        \n",
    "# Create the training and testing dataset.  Words will be replaced with indexes for the words        \n",
    "X_train = tok.texts_to_sequences(input_text[:25000])\n",
    "X_test  = tok.texts_to_sequences(input_text[25000:])\n",
    "y_train = input_label[:25000]\n",
    "y_test  = input_label[25000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addtional Data Processing\n",
    "\n",
    "We will pad the data to a fixed length and create NDArrayIter to be used for training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabsize = num_words\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "\n",
    "# Specify the maximum length of the reviews we want to process and pad the training and test data \n",
    "maxtextlen = 500\n",
    "X_train = pad_sequences(X_train, maxlen=maxtextlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxtextlen)\n",
    "\n",
    "# convert list to nd array type as mx.io.NDArrayIter takes nd array data type\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Create MXNet NDArray Iterators from the numpy training set and labels.  A batch size specified and the data will\n",
    "# be shffled.  The iterators will be used as input to train and measure the model performance later.\n",
    "Batch_Size = 50\n",
    "trainIter = mx.io.NDArrayIter(X_train, y_train, Batch_Size, shuffle=True)\n",
    "testIter = mx.io.NDArrayIter(X_test, y_test, Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summarization \n",
    "\n",
    "Let's take a look at some of the basic metrics of the datasets including number of unique words, unique label values, and the mean and standard deviation of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 9999\n",
      "\n",
      "Label value\n",
      "[0 1]\n",
      "\n",
      "Review length: \n",
      "Mean 216.85 words (158.170289)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x0viqf0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3cdfabf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's do some analysis of the data\n",
    "# Summarize review length\n",
    "print(\"Number of unique words : %i\" % len(np.unique(np.hstack(X))))\n",
    "print ('')\n",
    "print (\"Label value\")\n",
    "print (np.unique(y_train))\n",
    "print ('')\n",
    "print(\"Review length: \")\n",
    "\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "\n",
    "# plot review length distribution\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Example - Coded with word index\n",
      "[ [8, 182, 4309, 8719, 104, 22, 239, 4, 565, 38, 713, 15, 87, 1217, 10, 18, 76, 25, 53, 1395, 5, 11, 2669, 3946, 2667, 1418, 13, 3, 751, 5, 8719, 35, 6624, 1, 1146, 10, 54, 6, 19, 1229, 318, 252, 5, 3198, 30, 1, 206, 2372, 2234, 5995, 33, 6, 108, 30, 3728, 40, 270, 40, 28, 4, 23, 8796, 7, 1, 4, 28, 4, 10, 3946, 626, 52, 19, 31, 6755, 1950, 15, 1962, 6065, 519, 1, 18, 123, 74, 3, 223, 176, 7, 1, 329, 316, 44, 870, 73, 3, 333, 4, 4309, 6101, 7, 1, 497, 10, 6, 27, 20, 5, 25, 1045, 14, 1, 6101, 82, 33, 43, 60, 553, 4, 23, 18, 227, 3114, 21, 234, 177, 5, 813, 42, 23, 906, 104, 154, 652, 19, 10, 27, 257, 44, 21, 767, 106, 242, 9, 6738, 1, 366, 153, 59, 29, 207, 1572, 68, 4, 808, 398, 128, 1532, 1376]]\n"
     ]
    }
   ],
   "source": [
    "# Let's also take a look at 1 row of the training data\n",
    "# The integers represent a word in the original text \n",
    "print ('Review Example - Coded with word index')\n",
    "print (X[0:1, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CNN Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 50\n",
      "embedding dimensions 300\n",
      "convolution filters [3, 4, 5]\n",
      "dropout probability 0.2\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"326pt\" height=\"1006pt\"\n",
       " viewBox=\"0.00 0.00 326.00 1006.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1002)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1002 322,-1002 322,4 -4,4\"/>\n",
       "<!-- data -->\n",
       "<g id=\"node1\" class=\"node\"><title>data</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"black\" cx=\"159\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-25.3\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- vocab_embed -->\n",
       "<g id=\"node2\" class=\"node\"><title>vocab_embed</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"206,-152 112,-152 112,-94 206,-94 206,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">vocab_embed</text>\n",
       "</g>\n",
       "<!-- vocab_embed&#45;&gt;data -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>vocab_embed&#45;&gt;data</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-83.7443C159,-75.2043 159,-66.2977 159,-58.2479\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-93.8971 154.5,-83.897 159,-88.8971 159,-83.8971 159,-83.8971 159,-83.8971 159,-88.8971 163.5,-83.8971 159,-93.8971 159,-93.8971\"/>\n",
       "</g>\n",
       "<!-- reshape0 -->\n",
       "<g id=\"node3\" class=\"node\"><title>reshape0</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"black\" points=\"206,-246 112,-246 112,-188 206,-188 206,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-213.3\" font-family=\"Times,serif\" font-size=\"14.00\">reshape0</text>\n",
       "</g>\n",
       "<!-- reshape0&#45;&gt;vocab_embed -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>reshape0&#45;&gt;vocab_embed</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-177.744C159,-169.204 159,-160.298 159,-152.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-187.897 154.5,-177.897 159,-182.897 159,-177.897 159,-177.897 159,-177.897 159,-182.897 163.5,-177.897 159,-187.897 159,-187.897\"/>\n",
       "</g>\n",
       "<!-- convolution0 -->\n",
       "<g id=\"node4\" class=\"node\"><title>convolution0</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"94,-340 -7.10543e-15,-340 -7.10543e-15,-282 94,-282 94,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\">Convolution</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\">3x300/1, 100</text>\n",
       "</g>\n",
       "<!-- convolution0&#45;&gt;reshape0 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>convolution0&#45;&gt;reshape0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M89.1482,-275.378C100.849,-265.767 113.411,-255.448 124.611,-246.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.2123,-281.897 86.0832,-272.072 85.0759,-278.723 88.9395,-275.55 88.9395,-275.55 88.9395,-275.55 85.0759,-278.723 91.7959,-279.027 81.2123,-281.897 81.2123,-281.897\"/>\n",
       "</g>\n",
       "<!-- activation0 -->\n",
       "<g id=\"node5\" class=\"node\"><title>activation0</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"94,-434 -7.10543e-15,-434 -7.10543e-15,-376 94,-376 94,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\">relu</text>\n",
       "</g>\n",
       "<!-- activation0&#45;&gt;convolution0 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>activation0&#45;&gt;convolution0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-365.744C47,-357.204 47,-348.298 47,-340.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-375.897 42.5001,-365.897 47,-370.897 47.0001,-365.897 47.0001,-365.897 47.0001,-365.897 47,-370.897 51.5001,-365.897 47,-375.897 47,-375.897\"/>\n",
       "</g>\n",
       "<!-- pooling0 -->\n",
       "<g id=\"node6\" class=\"node\"><title>pooling0</title>\n",
       "<polygon fill=\"#80b1d3\" stroke=\"black\" points=\"94,-528 -7.10543e-15,-528 -7.10543e-15,-470 94,-470 94,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-502.8\" font-family=\"Times,serif\" font-size=\"14.00\">Pooling</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-487.8\" font-family=\"Times,serif\" font-size=\"14.00\">max, 498x1/1x1</text>\n",
       "</g>\n",
       "<!-- pooling0&#45;&gt;activation0 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>pooling0&#45;&gt;activation0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-459.744C47,-451.204 47,-442.298 47,-434.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-469.897 42.5001,-459.897 47,-464.897 47.0001,-459.897 47.0001,-459.897 47.0001,-459.897 47,-464.897 51.5001,-459.897 47,-469.897 47,-469.897\"/>\n",
       "</g>\n",
       "<!-- convolution1 -->\n",
       "<g id=\"node7\" class=\"node\"><title>convolution1</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"206,-340 112,-340 112,-282 206,-282 206,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\">Convolution</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\">4x300/1, 100</text>\n",
       "</g>\n",
       "<!-- convolution1&#45;&gt;reshape0 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>convolution1&#45;&gt;reshape0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-271.744C159,-263.204 159,-254.298 159,-246.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-281.897 154.5,-271.897 159,-276.897 159,-271.897 159,-271.897 159,-271.897 159,-276.897 163.5,-271.897 159,-281.897 159,-281.897\"/>\n",
       "</g>\n",
       "<!-- activation1 -->\n",
       "<g id=\"node8\" class=\"node\"><title>activation1</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"206,-434 112,-434 112,-376 206,-376 206,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\">relu</text>\n",
       "</g>\n",
       "<!-- activation1&#45;&gt;convolution1 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>activation1&#45;&gt;convolution1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-365.744C159,-357.204 159,-348.298 159,-340.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-375.897 154.5,-365.897 159,-370.897 159,-365.897 159,-365.897 159,-365.897 159,-370.897 163.5,-365.897 159,-375.897 159,-375.897\"/>\n",
       "</g>\n",
       "<!-- pooling1 -->\n",
       "<g id=\"node9\" class=\"node\"><title>pooling1</title>\n",
       "<polygon fill=\"#80b1d3\" stroke=\"black\" points=\"206,-528 112,-528 112,-470 206,-470 206,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-502.8\" font-family=\"Times,serif\" font-size=\"14.00\">Pooling</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-487.8\" font-family=\"Times,serif\" font-size=\"14.00\">max, 497x1/1x1</text>\n",
       "</g>\n",
       "<!-- pooling1&#45;&gt;activation1 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>pooling1&#45;&gt;activation1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-459.744C159,-451.204 159,-442.298 159,-434.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-469.897 154.5,-459.897 159,-464.897 159,-459.897 159,-459.897 159,-459.897 159,-464.897 163.5,-459.897 159,-469.897 159,-469.897\"/>\n",
       "</g>\n",
       "<!-- convolution2 -->\n",
       "<g id=\"node10\" class=\"node\"><title>convolution2</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"318,-340 224,-340 224,-282 318,-282 318,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\">Convolution</text>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\">5x300/1, 100</text>\n",
       "</g>\n",
       "<!-- convolution2&#45;&gt;reshape0 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>convolution2&#45;&gt;reshape0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M228.852,-275.378C217.151,-265.767 204.589,-255.448 193.389,-246.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.788,-281.897 226.204,-279.027 232.924,-278.723 229.06,-275.55 229.06,-275.55 229.06,-275.55 232.924,-278.723 231.917,-272.072 236.788,-281.897 236.788,-281.897\"/>\n",
       "</g>\n",
       "<!-- activation2 -->\n",
       "<g id=\"node11\" class=\"node\"><title>activation2</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"318,-434 224,-434 224,-376 318,-376 318,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\">relu</text>\n",
       "</g>\n",
       "<!-- activation2&#45;&gt;convolution2 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>activation2&#45;&gt;convolution2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271,-365.744C271,-357.204 271,-348.298 271,-340.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271,-375.897 266.5,-365.897 271,-370.897 271,-365.897 271,-365.897 271,-365.897 271,-370.897 275.5,-365.897 271,-375.897 271,-375.897\"/>\n",
       "</g>\n",
       "<!-- pooling2 -->\n",
       "<g id=\"node12\" class=\"node\"><title>pooling2</title>\n",
       "<polygon fill=\"#80b1d3\" stroke=\"black\" points=\"318,-528 224,-528 224,-470 318,-470 318,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-502.8\" font-family=\"Times,serif\" font-size=\"14.00\">Pooling</text>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-487.8\" font-family=\"Times,serif\" font-size=\"14.00\">max, 496x1/1x1</text>\n",
       "</g>\n",
       "<!-- pooling2&#45;&gt;activation2 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>pooling2&#45;&gt;activation2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M271,-459.744C271,-451.204 271,-442.298 271,-434.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"271,-469.897 266.5,-459.897 271,-464.897 271,-459.897 271,-459.897 271,-459.897 271,-464.897 275.5,-459.897 271,-469.897 271,-469.897\"/>\n",
       "</g>\n",
       "<!-- concat0 -->\n",
       "<g id=\"node13\" class=\"node\"><title>concat0</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"black\" points=\"206,-622 112,-622 112,-564 206,-564 206,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\">concat0</text>\n",
       "</g>\n",
       "<!-- concat0&#45;&gt;pooling0 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>concat0&#45;&gt;pooling0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M116.852,-557.378C105.151,-547.767 92.5885,-537.448 81.3887,-528.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.788,-563.897 114.204,-561.027 120.924,-560.723 117.06,-557.55 117.06,-557.55 117.06,-557.55 120.924,-560.723 119.917,-554.072 124.788,-563.897 124.788,-563.897\"/>\n",
       "</g>\n",
       "<!-- concat0&#45;&gt;pooling1 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>concat0&#45;&gt;pooling1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-553.744C159,-545.204 159,-536.298 159,-528.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-563.897 154.5,-553.897 159,-558.897 159,-553.897 159,-553.897 159,-553.897 159,-558.897 163.5,-553.897 159,-563.897 159,-563.897\"/>\n",
       "</g>\n",
       "<!-- concat0&#45;&gt;pooling2 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>concat0&#45;&gt;pooling2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M201.148,-557.378C212.849,-547.767 225.411,-537.448 236.611,-528.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"193.212,-563.897 198.083,-554.072 197.076,-560.723 200.94,-557.55 200.94,-557.55 200.94,-557.55 197.076,-560.723 203.796,-561.027 193.212,-563.897 193.212,-563.897\"/>\n",
       "</g>\n",
       "<!-- reshape1 -->\n",
       "<g id=\"node14\" class=\"node\"><title>reshape1</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"black\" points=\"206,-716 112,-716 112,-658 206,-658 206,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-683.3\" font-family=\"Times,serif\" font-size=\"14.00\">reshape1</text>\n",
       "</g>\n",
       "<!-- reshape1&#45;&gt;concat0 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>reshape1&#45;&gt;concat0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-647.744C159,-639.204 159,-630.298 159,-622.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-657.897 154.5,-647.897 159,-652.897 159,-647.897 159,-647.897 159,-647.897 159,-652.897 163.5,-647.897 159,-657.897 159,-657.897\"/>\n",
       "</g>\n",
       "<!-- dropout0 -->\n",
       "<g id=\"node15\" class=\"node\"><title>dropout0</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"206,-810 112,-810 112,-752 206,-752 206,-810\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-777.3\" font-family=\"Times,serif\" font-size=\"14.00\">dropout0</text>\n",
       "</g>\n",
       "<!-- dropout0&#45;&gt;reshape1 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>dropout0&#45;&gt;reshape1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-741.744C159,-733.204 159,-724.298 159,-716.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-751.897 154.5,-741.897 159,-746.897 159,-741.897 159,-741.897 159,-741.897 159,-746.897 163.5,-741.897 159,-751.897 159,-751.897\"/>\n",
       "</g>\n",
       "<!-- fullyconnected0 -->\n",
       "<g id=\"node16\" class=\"node\"><title>fullyconnected0</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"206,-904 112,-904 112,-846 206,-846 206,-904\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-878.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-863.8\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- fullyconnected0&#45;&gt;dropout0 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>fullyconnected0&#45;&gt;dropout0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-835.744C159,-827.204 159,-818.298 159,-810.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-845.897 154.5,-835.897 159,-840.897 159,-835.897 159,-835.897 159,-835.897 159,-840.897 163.5,-835.897 159,-845.897 159,-845.897\"/>\n",
       "</g>\n",
       "<!-- softmax_label -->\n",
       "<g id=\"node17\" class=\"node\"><title>softmax_label</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"black\" cx=\"271\" cy=\"-875\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-871.3\" font-family=\"Times,serif\" font-size=\"14.00\">softmax_label</text>\n",
       "</g>\n",
       "<!-- softmax -->\n",
       "<g id=\"node18\" class=\"node\"><title>softmax</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"262,-998 168,-998 168,-940 262,-940 262,-998\"/>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-965.3\" font-family=\"Times,serif\" font-size=\"14.00\">softmax</text>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;fullyconnected0 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>softmax&#45;&gt;fullyconnected0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.569,-931.148C187.113,-922.186 181.363,-912.74 176.194,-904.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.894,-939.897 188.85,-933.695 195.294,-935.626 192.694,-931.355 192.694,-931.355 192.694,-931.355 195.294,-935.626 196.538,-929.015 197.894,-939.897 197.894,-939.897\"/>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;softmax_label -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>softmax&#45;&gt;softmax_label</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.377,-931.237C243.257,-921.578 249.481,-911.353 254.955,-902.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"232.106,-939.897 233.462,-929.015 234.706,-935.626 237.306,-931.355 237.306,-931.355 237.306,-931.355 234.706,-935.626 241.15,-933.695 232.106,-939.897 232.106,-939.897\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fc3cdf3bb90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Define batch size and the place holders for network inputs and outputs\n",
    "'''\n",
    "\n",
    "batch_size = Batch_Size # the size of batches to train network with\n",
    "print 'batch size', batch_size\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "\n",
    "\n",
    "'''\n",
    "Define the first network layer (embedding)\n",
    "'''\n",
    "\n",
    "vocab_size = num_words\n",
    "sentence_size = maxtextlen\n",
    "\n",
    "# create embedding layer to learn representation of words in a lower dimensional subspace (much like word2vec)\n",
    "num_embed = 300 # dimensions to embed words into\n",
    "print 'embedding dimensions', num_embed\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "# reshape embedded data for next layer\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, target_shape=(batch_size, 1, sentence_size, num_embed))\n",
    "\n",
    "\n",
    "# create convolution + (max) pooling layer for each filter operation\n",
    "filter_list=[3, 4, 5] # the size of filters to use\n",
    "print 'convolution filters', filter_list\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_list):\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1,1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, target_shape=(batch_size, total_filters))\n",
    "\n",
    "\n",
    "# dropout layer\n",
    "dropout=0.2\n",
    "print 'dropout probability', dropout\n",
    "\n",
    "if dropout > 0.0:\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "else:\n",
    "    h_drop = h_pool\n",
    "    \n",
    "# fully connected layer\n",
    "num_label=2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_drop, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm\n",
    "\n",
    "mx.viz.plot_network(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Now we are ready to train the model.  We also need to define some hyper-parameters for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs to run\n",
    "num_epoch = 5\n",
    "\n",
    "# Assign the network symbol(mlp) to the module class and we will use gpu here.  If cpu is used, then change it \n",
    "# content = mx.cpu()\n",
    "cnn_model = mx.mod.Module(symbol=cnn, context=mx.gpu()) \n",
    "\n",
    "\n",
    "# Start training by calling the fit function\n",
    "cnn_model.fit(trainIter,  # training data               \n",
    "    eval_data=testIter,  # validation data                            \n",
    "    optimizer=\"adam\",  # use adam optimizer to train\n",
    "    optimizer_params={'learning_rate':0.01}, # set learning rate for adam         \n",
    "    eval_metric='acc',  # report accuracy during training  \n",
    "    batch_end_callback = mx.callback.Speedometer(Batch_Size, 100), # output progress for each 100 data batches   \n",
    "    num_epoch=num_epoch) # train data passes indicatd by num_epoch\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "We already evaluate the model during training for each epoch.  Let's also show how to evaluat the model separately from the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mse', 0.47619396555423738), ('accuracy', 0.84372)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "cnn_model.score(testIter, metric)\n",
    "cnn_model.score(testIter, ['mse', 'acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving The Model\n",
    "\n",
    "Now we have the model fully trained, we can save the model for reuse later. \n",
    "2 files will be generated. \n",
    "json file captures the network configuration of the neural network,\n",
    "params file captures the learned parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved checkpoint to \"sentiment_cnn_-0005.params\"\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "prefix = \"sentiment_cnn_\"\n",
    "cnn_model.save_checkpoint (prefix, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Prediction\n",
    "\n",
    "There are 3 steps to making prediction\n",
    "- Load the saved model\n",
    "- Prepare the input data\n",
    "- Make predictiokn with the input data and loaded model\n",
    "\n",
    "Also we will be using some UI widgets, so run the command below on the server to enable widget extension. \n",
    "- jupyter nbextension enable --py --sys-prefix widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model\n",
    "Load the model we just saved, and we will use this model for prediction.  We can also directly use the trained model instead.  Here we just want to demonstrate the model saving and loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/mxnet-0.10.0-py2.7.egg/mxnet/module/base_module.py:64: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Let's make some prediction using the saved model\n",
    "# First load the model\n",
    "num_epoch = 5\n",
    "maxtextlen = 500\n",
    "prefix = \"sentiment_cnn_\"\n",
    "model = mx.mod.Module.load(prefix, num_epoch, False)\n",
    "\n",
    "# Now we need to bind the model with a datashape that represents the input, which will be 1 x maxtextlen\n",
    "model.bind(for_training=False, data_shapes=[('data',(Batch_Size, maxtextlen ) )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper function for making the prediction\n",
    "\n",
    "# This function takes a text string and return a nd array with word indexes \n",
    "def prepare_imdb_list(text, maxlen=500, vocabsize=10000, batch_size=1):\n",
    "    imdb_word_index = tok.word_index\n",
    "    \n",
    "    sentence = []\n",
    "\n",
    "    sentence.append(str(text))\n",
    "    \n",
    "\n",
    "    #tokenize the input sentence\n",
    "    tokens = Tokenizer()\n",
    "    tokens.fit_on_texts(sentence)\n",
    "\n",
    "    # get a list of words from the encoding\n",
    "    words = []\n",
    "    for iter in range(len(tokens.word_index)):\n",
    "        words += [key for key,value in tokens.word_index.items() if value==iter+1]\n",
    "    \n",
    "    # create a imdb based sequence from the words and specified vocab size\n",
    "    imdb_seq = []\n",
    "    for w in words:\n",
    "        idx = imdb_word_index[w]\n",
    "        if idx < vocabsize:\n",
    "            imdb_seq.append(idx)\n",
    "\n",
    "    # next we need to create a list of list so we can use pad_sequence to pad the inputs\n",
    "    new_list = []\n",
    "    new_list.append(imdb_seq)\n",
    "    new_list = pad_sequences(new_list, maxlen=maxlen)\n",
    "    \n",
    "    # Now we need to create a batch for the input based on the batch size to match the shape of input with the target shape of the model\n",
    "    batch_list = []\n",
    "    for i in range(batch_size):\n",
    "        batch_list.append(new_list)\n",
    "    \n",
    "    batch_list = np.asarray(batch_list)    \n",
    "    batch_list = np.reshape(batch_list, (batch_size, maxlen))\n",
    "                            \n",
    "    return batch_list\n",
    "\n",
    "\n",
    "def predict_sentiment(model, text_nd, batch_size):\n",
    "    sentence_Iter = mx.io.NDArrayIter(text_nd, batch_size=batch_size)\n",
    "    pred = model.predict(sentence_Iter)\n",
    "    return pred\n",
    "\n",
    "def handle_submit(sender):\n",
    "    text_nd = prepare_imdb_list(inputtext.value, vocabsize=vocabsize, batch_size=Batch_Size)\n",
    "    pred = predict_sentiment(model, text_nd, Batch_Size)\n",
    "    outputlabel_0.value = 'Probability for negative sentiment (0):  %0.4f ' % pred.asnumpy()[0:1,0]\n",
    "    outputlabel_1.value = 'Probability for positive sentiment (1):   %0.4f ' % pred.asnumpy()[0:1,1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Movie Review Text For Testing\n",
    "\n",
    "Enter samples below or other review text to test the predictive power of the trained model. \n",
    "\n",
    "Negative sentiment review samples\n",
    "- Blake Edwards' legendary fiasco, begins to seem pointless after just 10 minutes. A combination of The Eagle Has Landed, Star!, Oh! What a Lovely War!, and Edwards' Pink Panther films, Darling Lili never engages the viewer; the aerial sequences, the musical numbers, the romance, the comedy, and the espionage are all ho hum. At what point is the viewer supposed to give a damn? This disaster wavers in tone, never decides what it wants to be, and apparently thinks it's a spoof, but it's pathetically and grindingly square. Old fashioned in the worst sense, audiences understandably stayed away in droves. It's awful. James Garner would have been a vast improvement over Hudson who is just cardboard, and he doesn't connect with Andrews and vice versa. And both Andrews and Hudson don't seem to have been let in on the joke and perform with a miscalculated earnestness. Blake Edwards' SOB isn't much more than OK, but it's the only good that ever came out of Darling Lili. The expensive and professional look of much of Darling Lili, only make what it's all lavished on even more difficult to bear. To quote Paramount chief Robert Evans, 24 million dollars worth of film and no picture.\n",
    "\n",
    "- A mean spirited, repulsive horror film about 3 murderous children. Susan Strasberg is totally wasted in a 5-minute cameo, even though she receives star billing. If your a Julie Brown fan, you'll want to check it out, since she's naked in a couple of shots. All others,avoid.\n",
    "\n",
    "\n",
    "Positive sentiment review samples\n",
    "- I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
    "\n",
    "- This is one of my three all-time favorite movies. My only quibble is that the director, Peter Yates, had too many cuts showing the actors individually instead of together as a scene, but the performances were so great I forgive him. Albert Finney and Tom are absolutely marvelous; brilliant. The script is great, giving a very good picture of life in the theatre during World War II (and, therefore, what it was like in the 30s as well). Lots of great, subtle touches, lots of broad, overplayed strokes, all of it perfectly done. Scene after scene just blows me away, and then there's the heartbreaking climax.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Screen For Model Testing\n",
    "\n",
    "Copy text into the text area and hit the 'Predict Sentiment' button to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2808a90d934e01900eed91d5d9a30c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcd1d8231b748f7bd181dde499fd948"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005e2f12189147889ed0b671d9979526"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513c885722644ae89ab1c16cee5c4280"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputtext = widgets.Textarea()\n",
    "\n",
    "display(inputtext)\n",
    "\n",
    "inputbutton = widgets.Button(description='Predict Sentiment')\n",
    "\n",
    "display(inputbutton)\n",
    "\n",
    "outputlabel_0 = widgets.HTML()\n",
    "outputlabel_1 = widgets.HTML()\n",
    "display(outputlabel_0)\n",
    "display(outputlabel_1)\n",
    "\n",
    "inputbutton.on_click(handle_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
