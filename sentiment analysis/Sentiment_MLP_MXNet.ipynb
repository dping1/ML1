{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling sentiment analysis in MXNet with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is MLP\n",
    "\n",
    "A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem \n",
    "\n",
    "Sentimenet ananlysis is the use of natual langurage processing(NLP) to determine the attitute expressed by an author in a piece of written text towards a topic, e.g. movie review.  The attitute can be positive, neutral, and negative.  \n",
    "\n",
    "From a machine learning perspective, sentiment analysis can be treated as a claffisication problem. In the tutorial, we will train a MLP based model for sentiment analysis.  \n",
    "\n",
    "While there are other algorithms, such as Recursive Neural Network or Recurrent Neural Network, that are better at capture the syntactic structure of the sentence for sentiment analysis.  MLP is a straight and simple network that's quick to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use This Tutorial¶\n",
    "You can use this tutorial by executing each snippet of python code in order as it appears in the notebook.\n",
    "In this tutorial, we will train a MLP on IMDB dataset which will ultimately produce a neural network that can predict the sentiment of movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Familiarity with MXNet, Python, Numpy, basics of MLP networks.\n",
    "AWS Deep Learning AMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview\n",
    "\n",
    "The training and testing dataset is the IMDB movie review database.  It contains a total of 50,000 movie reviews that are tagged (labelled) with either a negative(0) or a positive(1) sentiment.  We will split the dataset into 25,000 reviews for training and 25,000 reviews for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load all the libraries and modules\n",
    "import copy\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "from text import Tokenizer\n",
    "import mxnet as mx\n",
    "from matplotlib import pyplot\n",
    "from six.moves.urllib.request import urlopen\n",
    "from sequence import pad_sequences\n",
    "\n",
    "from IPython.display import display \n",
    "from IPython.html import widgets\n",
    "\n",
    "# Enable logging so we will see output during the training\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Movie Review Data\n",
    "\n",
    "The dataset can be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/. We will process the unzipped raw reviews into traing and testing datasets for training and validation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# We specify number of words to index and this is also the size of vocabulary\n",
    "num_words = 10000\n",
    "\n",
    "# This is the directory where the raw review data is located\n",
    "path = \"../data/aclImdb/\"\n",
    "\n",
    "# List all the files for the reviews in the following directories\n",
    "ff = [path + \"train/pos/\" + x for x in os.listdir(path + \"train/pos\")] + \\\n",
    "     [path + \"train/neg/\" + x for x in os.listdir(path + \"train/neg\")] + \\\n",
    "     [path + \"test/pos/\" + x for x in os.listdir(path + \"test/pos\")] + \\\n",
    "     [path + \"test/neg/\" + x for x in os.listdir(path + \"test/neg\")]\n",
    "\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "input_label = ([1] * 12500 + [0] * 12500) * 2\n",
    "input_text  = []\n",
    "\n",
    "for f in ff:\n",
    "    with open(f) as fin:\n",
    "        pass\n",
    "        input_text += [remove_tags(\" \".join(fin.readlines()))]\n",
    "            \n",
    "# Initialize a tokenizer with the vocabulary size and train on data input text to create a vocabulary for all \n",
    "# the unique words found in the text inputs\n",
    "tok = Tokenizer(num_words)\n",
    "tok.fit_on_texts(input_text[:25000])\n",
    "    \n",
    "        \n",
    "# Create the training and testing dataset.  Words will be replaced with indexes for the words        \n",
    "X_train = tok.texts_to_sequences(input_text[:25000])\n",
    "X_test  = tok.texts_to_sequences(input_text[25000:])\n",
    "y_train = input_label[:25000]\n",
    "y_test  = input_label[25000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addtional Data Processing\n",
    "\n",
    "We will pad the data to a fixed length and create NDArrayIter to be used for training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabsize = num_words\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "\n",
    "# Specify the maximum length of the reviews we want to process and pad the training and test data \n",
    "maxtextlen = 500\n",
    "X_train = pad_sequences(X_train, maxlen=maxtextlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxtextlen)\n",
    "\n",
    "# convert list to nd array type as mx.io.NDArrayIter takes nd array data type\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Create MXNet NDArray Iterators from the numpy training set and labels.  A batch size specified and the data will\n",
    "# be shffled.  The iterators will be used as input to train and measure the model performance later.\n",
    "Batch_Size = 250\n",
    "trainIter = mx.io.NDArrayIter(X_train, y_train, Batch_Size, shuffle=True)\n",
    "testIter = mx.io.NDArrayIter(X_test, y_test, Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summarization \n",
    "\n",
    "Let's take a look at some of the basic metrics of the datasets including number of unique words, unique label values, and the mean and standard deviation of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 9999\n",
      "\n",
      "Label value\n",
      "[0 1]\n",
      "\n",
      "Review length: \n",
      "Mean 216.85 words (158.170289)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x0viqf0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f0cf75dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's do some analysis of the data\n",
    "# Summarize review length\n",
    "print(\"Number of unique words : %i\" % len(np.unique(np.hstack(X))))\n",
    "print ('')\n",
    "print (\"Label value\")\n",
    "print (np.unique(y_train))\n",
    "print ('')\n",
    "print(\"Review length: \")\n",
    "\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "\n",
    "# plot review length distribution\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Example - Coded with word index\n",
      "[ [8, 182, 4309, 8719, 104, 22, 239, 4, 565, 38, 713, 15, 87, 1217, 10, 18, 76, 25, 53, 1395, 5, 11, 2669, 3946, 2667, 1418, 13, 3, 751, 5, 8719, 35, 6624, 1, 1146, 10, 54, 6, 19, 1229, 318, 252, 5, 3198, 30, 1, 206, 2372, 2234, 5995, 33, 6, 108, 30, 3728, 40, 270, 40, 28, 4, 23, 8796, 7, 1, 4, 28, 4, 10, 3946, 626, 52, 19, 31, 6755, 1950, 15, 1962, 6065, 519, 1, 18, 123, 74, 3, 223, 176, 7, 1, 329, 316, 44, 870, 73, 3, 333, 4, 4309, 6101, 7, 1, 497, 10, 6, 27, 20, 5, 25, 1045, 14, 1, 6101, 82, 33, 43, 60, 553, 4, 23, 18, 227, 3114, 21, 234, 177, 5, 813, 42, 23, 906, 104, 154, 652, 19, 10, 27, 257, 44, 21, 767, 106, 242, 9, 6738, 1, 366, 153, 59, 29, 207, 1572, 68, 4, 808, 398, 128, 1532, 1376]]\n"
     ]
    }
   ],
   "source": [
    "# Let's also take a look at 1 row of the training data\n",
    "# The integers represent a word in the original text \n",
    "print ('Review Example - Coded with word index')\n",
    "print (X[0:1, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a MLP Network\n",
    "\n",
    "We will build a simple MLP network with 2 hidden layers, and a softmax classifer to determine the probability of negative and positive sentiment in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout probability 0.5\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"214pt\" height=\"724pt\"\n",
       " viewBox=\"0.00 0.00 214.00 724.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 720)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-720 210,-720 210,4 -4,4\"/>\n",
       "<!-- data -->\n",
       "<g id=\"node1\" class=\"node\"><title>data</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"black\" cx=\"47\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-25.3\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- embed -->\n",
       "<g id=\"node2\" class=\"node\"><title>embed</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"94,-152 -7.10543e-15,-152 -7.10543e-15,-94 94,-94 94,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">embed</text>\n",
       "</g>\n",
       "<!-- embed&#45;&gt;data -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>embed&#45;&gt;data</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-83.7443C47,-75.2043 47,-66.2977 47,-58.2479\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-93.8971 42.5001,-83.897 47,-88.8971 47.0001,-83.8971 47.0001,-83.8971 47.0001,-83.8971 47,-88.8971 51.5001,-83.8971 47,-93.8971 47,-93.8971\"/>\n",
       "</g>\n",
       "<!-- flatten -->\n",
       "<g id=\"node3\" class=\"node\"><title>flatten</title>\n",
       "<polygon fill=\"#fdb462\" stroke=\"black\" points=\"94,-246 -7.10543e-15,-246 -7.10543e-15,-188 94,-188 94,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-213.3\" font-family=\"Times,serif\" font-size=\"14.00\">flatten</text>\n",
       "</g>\n",
       "<!-- flatten&#45;&gt;embed -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>flatten&#45;&gt;embed</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-177.744C47,-169.204 47,-160.298 47,-152.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-187.897 42.5001,-177.897 47,-182.897 47.0001,-177.897 47.0001,-177.897 47.0001,-177.897 47,-182.897 51.5001,-177.897 47,-187.897 47,-187.897\"/>\n",
       "</g>\n",
       "<!-- fullyconnected0 -->\n",
       "<g id=\"node4\" class=\"node\"><title>fullyconnected0</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"94,-340 -7.10543e-15,-340 -7.10543e-15,-282 94,-282 94,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\">250</text>\n",
       "</g>\n",
       "<!-- fullyconnected0&#45;&gt;flatten -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>fullyconnected0&#45;&gt;flatten</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-271.744C47,-263.204 47,-254.298 47,-246.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-281.897 42.5001,-271.897 47,-276.897 47.0001,-271.897 47.0001,-271.897 47.0001,-271.897 47,-276.897 51.5001,-271.897 47,-281.897 47,-281.897\"/>\n",
       "</g>\n",
       "<!-- activation0 -->\n",
       "<g id=\"node5\" class=\"node\"><title>activation0</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"94,-434 -7.10543e-15,-434 -7.10543e-15,-376 94,-376 94,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\">relu</text>\n",
       "</g>\n",
       "<!-- activation0&#45;&gt;fullyconnected0 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>activation0&#45;&gt;fullyconnected0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-365.744C47,-357.204 47,-348.298 47,-340.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-375.897 42.5001,-365.897 47,-370.897 47.0001,-365.897 47.0001,-365.897 47.0001,-365.897 47,-370.897 51.5001,-365.897 47,-375.897 47,-375.897\"/>\n",
       "</g>\n",
       "<!-- dropout0 -->\n",
       "<g id=\"node6\" class=\"node\"><title>dropout0</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"94,-528 -7.10543e-15,-528 -7.10543e-15,-470 94,-470 94,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-495.3\" font-family=\"Times,serif\" font-size=\"14.00\">dropout0</text>\n",
       "</g>\n",
       "<!-- dropout0&#45;&gt;activation0 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>dropout0&#45;&gt;activation0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-459.744C47,-451.204 47,-442.298 47,-434.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-469.897 42.5001,-459.897 47,-464.897 47.0001,-459.897 47.0001,-459.897 47.0001,-459.897 47,-464.897 51.5001,-459.897 47,-469.897 47,-469.897\"/>\n",
       "</g>\n",
       "<!-- fullyconnected1 -->\n",
       "<g id=\"node7\" class=\"node\"><title>fullyconnected1</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"94,-622 -7.10543e-15,-622 -7.10543e-15,-564 94,-564 94,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-596.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-581.8\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- fullyconnected1&#45;&gt;dropout0 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>fullyconnected1&#45;&gt;dropout0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47,-553.744C47,-545.204 47,-536.298 47,-528.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47,-563.897 42.5001,-553.897 47,-558.897 47.0001,-553.897 47.0001,-553.897 47.0001,-553.897 47,-558.897 51.5001,-553.897 47,-563.897 47,-563.897\"/>\n",
       "</g>\n",
       "<!-- softmax_label -->\n",
       "<g id=\"node8\" class=\"node\"><title>softmax_label</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"black\" cx=\"159\" cy=\"-593\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\">softmax_label</text>\n",
       "</g>\n",
       "<!-- softmax -->\n",
       "<g id=\"node9\" class=\"node\"><title>softmax</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"150,-716 56,-716 56,-658 150,-658 150,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-683.3\" font-family=\"Times,serif\" font-size=\"14.00\">softmax</text>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;fullyconnected1 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>softmax&#45;&gt;fullyconnected1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.5686,-649.148C75.113,-640.186 69.3635,-630.74 64.1943,-622.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.8939,-657.897 76.8505,-651.695 83.2941,-653.626 80.6944,-649.355 80.6944,-649.355 80.6944,-649.355 83.2941,-653.626 84.5383,-647.015 85.8939,-657.897 85.8939,-657.897\"/>\n",
       "</g>\n",
       "<!-- softmax&#45;&gt;softmax_label -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>softmax&#45;&gt;softmax_label</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.377,-649.237C131.257,-639.578 137.481,-629.353 142.955,-620.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.106,-657.897 121.462,-647.015 122.706,-653.626 125.306,-649.355 125.306,-649.355 125.306,-649.355 122.706,-653.626 129.15,-651.695 120.106,-657.897 120.106,-657.897\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2ea3c75810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create MLP network using MXNet Symbol API\n",
    "\n",
    "# Create the input layer and place holder for the label\n",
    "inputdata = mx.sym.Variable('data')\n",
    "input_y = mx.sym.Variable('softmax_label')  # placeholder for label\n",
    "\n",
    "# We embed the integer representation for each word into a vector of size 32.  Embedding is a technique that help \n",
    "# place related words close together. This helps improve the accuracy of model\n",
    "# input_dim is the size of the vocabulary.  output_dim is the dimension of the output embedded vector.\n",
    "Embeddata = mx.sym.Embedding(data = inputdata, input_dim=vocabsize, output_dim=32, name='embed') \n",
    "\n",
    "# The output from the embedding layer will be dimensional matrix, since MLP only accepts 1 dimensional vector, \n",
    "# we need to flatten it back to one dimension vector\n",
    "data1 = mx.sym.Flatten(data = Embeddata, name='flatten')\n",
    "\n",
    "\n",
    "# We create a fully connected layer with 250 neurons.  This layer will take the flattened input and \n",
    "# perform a linear calculation on the input data f(x) = ⟨w, x⟩ + b\n",
    "fc1  = mx.sym.FullyConnected(data=data1, num_hidden=250)\n",
    "\n",
    "# We add some nonlearity (Activation) into the network, so we can model non linear data patterns as not problem is linear problem\n",
    "# Some of the common activations functions are 'relu', 'tanh', sigmoid.  \n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")  \n",
    "\n",
    "# dropout layer\n",
    "dropout=0.5\n",
    "print 'dropout probability', dropout\n",
    "\n",
    "if dropout > 0.0:\n",
    "    h_drop = mx.sym.Dropout(data=act1, p=dropout)\n",
    "else:\n",
    "    h_drop = act1\n",
    "\n",
    "# We create anothe hidden layer with 2 hidden units as we have 2 desired output (1, 0)\n",
    "fc2 = mx.sym.FullyConnected(data=h_drop, num_hidden=2) \n",
    "\n",
    "# Softmax is a classifier, and cross-entropy loss is used as the loss function by default.  \n",
    "mlp = mx.sym.SoftmaxOutput(data=fc2, label=input_y, name='softmax')\n",
    "\n",
    "# Now we have completed building the network, let's see what it looks like\n",
    "mx.viz.plot_network(mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Now we are ready to train the model.  We also need to define some hyper-parameters for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs to run\n",
    "num_epoch = 10\n",
    "\n",
    "# Assign the network symbol(mlp) to the module class and we will use gpu here.  If cpu is used, then change it \n",
    "# content = mx.cpu()\n",
    "mlp_model = mx.mod.Module(symbol=mlp, context=mx.gpu()) \n",
    "\n",
    "\n",
    "# Start training by calling the fit function\n",
    "mlp_model.fit(trainIter,  # training data               \n",
    "    eval_data=testIter,  # validation data                            \n",
    "    optimizer=\"adam\",  # use adam optimizer to train\n",
    "    optimizer_params={'learning_rate':0.01}, # set learning rate for adam         \n",
    "    eval_metric='acc',  # report accuracy during training  \n",
    "    batch_end_callback = mx.callback.Speedometer(Batch_Size, 100), # output progress for each 100 data batches   \n",
    "    num_epoch=num_epoch) # train data passes indicatd by num_epoch\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "We already evaluated the model during training.  Let's also try evaluating the trained model separately from the traing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mse', 0.48084812015295031), ('accuracy', 0.8434)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "mlp_model.score(testIter, metric)\n",
    "mlp_model.score(testIter, ['mse', 'acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving The Model\n",
    "\n",
    "Now we have the model fully trained, we can save the model for reuse later\n",
    "2 files will be generated\n",
    "json file captures the network configuration of the neural network\n",
    "params file captures the learned parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved checkpoint to \"sentiment_mlp_-0010.params\"\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "prefix = \"sentiment_mlp_\"\n",
    "mlp_model.save_checkpoint (prefix, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Prediction\n",
    "\n",
    "Run command below on the server to enable widget extension as we need to use UI widgets\n",
    "- jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/mxnet-0.10.0-py2.7.egg/mxnet/module/base_module.py:64: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Let's make some prediction using the saved model\n",
    "# First load the model\n",
    "num_epoch = 10\n",
    "maxtextlen = 500\n",
    "prefix = \"sentiment_mlp_\"\n",
    "model = mx.mod.Module.load(prefix, num_epoch, False)\n",
    "\n",
    "# Now we need to bind the model with a datashape that represents the input, which will be 1xmaxtextlen\n",
    "model.bind(for_training=False, data_shapes=[('data', (1,maxtextlen))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper function for making the prediction\n",
    "\n",
    "# This function takes a text string and return a nd array with word indexes \n",
    "def prepare_imdb_list(text, maxlen=500, vocabsize=10000):\n",
    "    imdb_word_index = tok.word_index\n",
    "    \n",
    "    sentence = []\n",
    "\n",
    "    sentence.append(str(text))\n",
    "    \n",
    "\n",
    "    #tokenize the input sentence\n",
    "    tokens = Tokenizer()\n",
    "    tokens.fit_on_texts(sentence)\n",
    "\n",
    "    # get a list of words from the encoding\n",
    "    words = []\n",
    "    for iter in range(len(tokens.word_index)):\n",
    "        words += [key for key,value in tokens.word_index.items() if value==iter+1]\n",
    "    \n",
    "    # create a imdb based sequence from the words and specified vocab size\n",
    "    imdb_seq = []\n",
    "    for w in words:\n",
    "        idx = imdb_word_index[w]\n",
    "        if idx < vocabsize:\n",
    "            imdb_seq.append(idx)\n",
    "\n",
    "    # next we need to create a list of list so we can use pad_sequence to pad the inputs\n",
    "    new_list = []\n",
    "    new_list.append(imdb_seq)\n",
    "\n",
    "    new_list = pad_sequences(new_list, maxlen=maxlen)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "\n",
    "def predict_sentiment(model, text_nd):\n",
    "    sentence_Iter = mx.io.NDArrayIter(text_nd, batch_size=1)\n",
    "    pred = model.predict(sentence_Iter)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def handle_submit(sender):\n",
    "    text_nd = prepare_imdb_list(inputtext.value)\n",
    "    pred = predict_sentiment(model, text_nd)\n",
    "    outputlabel_0.value = 'Probability for negative sentiment (0):  %0.4f ' % pred.asnumpy()[0:1,0]\n",
    "    outputlabel_1.value = 'Probability for positive sentiment (1):   %0.4f ' % pred.asnumpy()[0:1,1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Movie Review Text For Testing\n",
    "\n",
    "Can use the samples below or other review text to try out the predictive power of the model. \n",
    "\n",
    "Negative sentiment review samples\n",
    "- Blake Edwards' legendary fiasco, begins to seem pointless after just 10 minutes. A combination of The Eagle Has Landed, Star!, Oh! What a Lovely War!, and Edwards' Pink Panther films, Darling Lili never engages the viewer; the aerial sequences, the musical numbers, the romance, the comedy, and the espionage are all ho hum. At what point is the viewer supposed to give a damn? This disaster wavers in tone, never decides what it wants to be, and apparently thinks it's a spoof, but it's pathetically and grindingly square. Old fashioned in the worst sense, audiences understandably stayed away in droves. It's awful. James Garner would have been a vast improvement over Hudson who is just cardboard, and he doesn't connect with Andrews and vice versa. And both Andrews and Hudson don't seem to have been let in on the joke and perform with a miscalculated earnestness. Blake Edwards' SOB isn't much more than OK, but it's the only good that ever came out of Darling Lili. The expensive and professional look of much of Darling Lili, only make what it's all lavished on even more difficult to bear. To quote Paramount chief Robert Evans, 24 million dollars worth of film and no picture.\n",
    "\n",
    "- A mean spirited, repulsive horror film about 3 murderous children. Susan Strasberg is totally wasted in a 5-minute cameo, even though she receives star billing. If your a Julie Brown fan, you'll want to check it out, since she's naked in a couple of shots. All others,avoid.\n",
    "\n",
    "\n",
    "Positive sentiment review samples\n",
    "- I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
    "\n",
    "- This is one of my three all-time favorite movies. My only quibble is that the director, Peter Yates, had too many cuts showing the actors individually instead of together as a scene, but the performances were so great I forgive him. Albert Finney and Tom are absolutely marvelous; brilliant. The script is great, giving a very good picture of life in the theatre during World War II (and, therefore, what it was like in the 30s as well). Lots of great, subtle touches, lots of broad, overplayed strokes, all of it perfectly done. Scene after scene just blows me away, and then there's the heartbreaking climax.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Screen For Model Testing\n",
    "\n",
    "Copy text into the text area and hit the 'Predict Sentiment' button to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3645d54c51254b1db8c6f59106bde080"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334f753a2e0b48059df266d49d57a775"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dc0751c9bb4d1ca1a5b08621409059"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f67c7fff3b44a92a4cfa74934d5ff5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputtext = widgets.Textarea()\n",
    "\n",
    "display(inputtext)\n",
    "\n",
    "inputbutton = widgets.Button(description='Predict Sentiment')\n",
    "\n",
    "display(inputbutton)\n",
    "\n",
    "outputlabel_0 = widgets.HTML()\n",
    "outputlabel_1 = widgets.HTML()\n",
    "display(outputlabel_0)\n",
    "display(outputlabel_1)\n",
    "\n",
    "inputbutton.on_click(handle_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
