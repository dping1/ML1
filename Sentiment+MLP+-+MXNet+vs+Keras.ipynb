{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB data using Keras \n",
    "\n",
    "import numpy\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what values are in the labels.  Should be 0 or 1.\n",
    "print (numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of unique words in the dataset\n",
    "print (len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# The number if unique words is too large.  For simplicity, we want to use the top 5000 words and zero out the rest.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test)  = imdb.load_data(nb_words=5000)\n",
    "\n",
    "# We need to pad the training data with 0's, so they all have lenghth of 500. As mojority of review fall into 500 words or less\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MXNet NDArray Iterator from the numpy training set and labels\n",
    "# in order to assign numpy array to MXNet NDArrayIter, the input vector needs to be same size and padded if it is short\n",
    "# 128 is the batch size.  We can choose to shuffle the data (true or false)\n",
    "trainIter = mx.io.NDArrayIter(X_train, y_train, 128, shuffle=True)\n",
    "testIter = mx.io.NDArrayIter(X_test, y_test, 128, shuffle=True)\n",
    "\n",
    "'''\n",
    "for batch in trainIter:\n",
    "    print ('Original Data')\n",
    "    print (X_train[0:128])\n",
    "    print ('Original Lable')\n",
    "    print (y_train[0:128])\n",
    "    print ('DATA')\n",
    "    print (batch.data[0].asnumpy().astype(int))\n",
    "    print ('LABEL')\n",
    "    print (batch.label[0].asnumpy().astype(int))\n",
    "    #print(batch.data, batch.label, batch.pad)\n",
    "    print ('EOL')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create MLP network using MXNet\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "inputdata = mx.sym.Variable('data')\n",
    "input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "\n",
    "#add a dropout.  Can be used for regularization\n",
    "#inputdatadp = mx.sym.Dropout(inputdata, p=0.2)\n",
    "\n",
    "vocabsize = 5000\n",
    "\n",
    "# input_dim is the size of the vocaburary.  output_dim is the dimension of the output embedded vector.\n",
    "Embeddata = mx.sym.Embedding(data = inputdata, input_dim=vocabsize, output_dim=32, name='embed') \n",
    "\n",
    "data1 = mx.sym.Flatten(data = Embeddata, name='flatten')\n",
    "\n",
    "fc1  = mx.sym.FullyConnected(data=data1, num_hidden=250) \n",
    "act1 = mx.sym.Activation(data=fc1, act_type=\"relu\")  \n",
    "\n",
    "\n",
    "fc2 = mx.sym.FullyConnected(data=act1, num_hidden=1) \n",
    "act2 = mx.sym.Activation(data=fc2, act_type=\"sigmoid\")  \n",
    "\n",
    "#mlp = mx.sym.SoftmaxOutput(data=act2, label=input_y, name='softmax')\n",
    "\n",
    "mlp = mx.sym.LogisticRegressionOutput(data=fc2, label=input_y, name='softmax')\n",
    "\n",
    "mx.viz.plot_network(mlp)\n",
    "\n",
    "#print (mlp.list_arguments())\n",
    "#print (mlp.list_outputs())\n",
    "#print (mlp.debug_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "\n",
    "def norm_stat(d):\n",
    "    \"\"\"The statistics you want to see.\n",
    "    We compute the L2 norm here but you can change it to anything you like.\"\"\"\n",
    "    return mx.nd.norm(d)/numpy.sqrt(d.size)\n",
    "\n",
    "\n",
    "# create a trainable module on GPU \n",
    "batch_size = 128\n",
    "\n",
    "mlp_model = mx.mod.Module(symbol=mlp, context=mx.gpu()) \n",
    "\n",
    "# bind is not needed for training using fit. Redudant  Keeping here to show warning.  \n",
    "#mlp_model.bind(data_shapes=trainIter.provide_data,label_shapes=trainIter.provide_label)\n",
    "\n",
    "# init_params is also optional for training using fit\n",
    "#mlp_model.init_params()\n",
    "\n",
    "mon = mx.mon.Monitor(\n",
    "    100,                 # Print every 100 batches\n",
    "    norm_stat,           # The statistics function defined above\n",
    "    pattern='.*weight',  # A regular expression. Only arrays with name matching this pattern will be included.\n",
    "    sort=True)           # Sort output by name\n",
    "\n",
    "\n",
    "\n",
    "mlp_model.fit(trainIter,  # training data               \n",
    "    #eval_data=testIter,  # validation data               \n",
    "    optimizer='sgd',  # use SGD to train               \n",
    "    optimizer_params={'learning_rate':0.01, 'momentum': 0.9},  # use fixed learning rate.  momentum is for sgd only              \n",
    "    #optimizer=\"adam\",  # use adam to train\n",
    "    #optimizer_params={'learning_rate':0.01}, # set learning rate for adam         \n",
    "    eval_metric='acc',  # report accuracy during training               \n",
    "    batch_end_callback = mx.callback.Speedometer(batch_size, 100), # output progress for each 100 data batches   \n",
    "    num_epoch=10, # train at most 10 data passes\n",
    "    monitor=mon)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated syntax.  \n",
    "# just trying it out\n",
    "\n",
    "model = mx.model.FeedForward(\n",
    "    ctx = mx.gpu(0),      # Run on GPU 0\n",
    "    symbol = mlp,         # Use the network we just defined\n",
    "    num_epoch = 10,       # Train for 10 epochs\n",
    "    learning_rate = 0.1,  # Learning rate\n",
    "    momentum = 0.9,       # Momentum for SGD with momentum\n",
    "    wd = 0.00001)         # Weight decay for regularization\n",
    "model.fit(\n",
    "    X=trainIter,  # Training data set\n",
    "    eval_data=testIter,  # Testing data set. MXNet computes scores on test set every epoch\n",
    "    batch_end_callback = mx.callback.Speedometer(batch_size, 200))  # Logging module to print out progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = mlp_model.predict(testIter) \n",
    "print (prob.shape)\n",
    "print (mlp_model)\n",
    "\n",
    "acc = mx.metric.Accuracy() \n",
    "mlp_model.score(testIter, acc) \n",
    "print(acc) \n",
    "print (acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import data_utils\n",
    "import json\n",
    "\n",
    "def get_word_index(path='imdb_word_index.json'):\n",
    "    \"\"\"Retrieves the dictionary mapping word indices back to words.\n",
    "    # Arguments\n",
    "        path: where to cache the data (relative to `~/.keras/dataset`).\n",
    "    # Returns\n",
    "        The word index dictionary.\n",
    "    \"\"\"\n",
    "    path = data_utils.get_file(path,\n",
    "                    origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.json')\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = get_word_index()\n",
    "\n",
    "# get json element directly with string as the index.  Use this to encode the sentence into numpy array\n",
    "print (data['timey']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
